# -*- coding: utf-8 -*-
"""major_p_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifZMv6GSWIb78psqIYEyWRzmNLczIJf1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

file_path ="/content/drive/My Drive/fraudTrain.csv"

file_path1 ="/content/drive/My Drive/fraudTest.csv"

# Load data
train_df = pd.read_csv(file_path)
test_df = pd.read_csv(file_path1)

train_df.shape

test_df.shape

df = pd.concat([train_df, test_df],ignore_index=True)

df.head()

df.shape

df.info()

df.isnull().sum()

df.is_fraud.value_counts()

df["trans_date_trans_time"] = pd.to_datetime(df["trans_date_trans_time"])

df.head()

# Convert 'trans_date_trans_time' to datetime format
df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])

# Extract useful features like year, month, day, hour, minute
df['year'] = df['trans_date_trans_time'].dt.year
df['month'] = df['trans_date_trans_time'].dt.month
df['day'] = df['trans_date_trans_time'].dt.day
df['hour'] = df['trans_date_trans_time'].dt.hour
df['minute'] = df['trans_date_trans_time'].dt.minute


# Drop the original 'trans_date_trans_time' column
df= df.drop(columns=['trans_date_trans_time'])

df.rename(columns={'Unnamed: 0': 'index',}, inplace=True)
df.drop('index', axis=1, inplace=True)
df.drop('first',axis=1,inplace=True)
df.drop('last',axis=1,inplace=True)
df.drop('dob',axis=1,inplace=True)

categorical_cols = [col for col in df.columns if df[col].dtype == 'object']
categorical_cols

df.nunique()

df.info()

print(df.columns)  # Check all column names

if 'gender' in df.columns:
    df['gender'] = LabelEncoder().fit_transform(df['gender'])
else:
    print("Column 'gender' not found!")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Ensure dataset is loaded in df

# **Step 1: Combine Category & Merchant into One Feature**
df['category_merchant'] = df['category'].astype(str) + "_" + df['merchant'].astype(str)
df.drop(columns=['category', 'merchant'], inplace=True)  # Drop original columns

# **Step 2: Convert Categorical Features Using Target Encoding**
categorical_cols = ['category_merchant', 'street', 'city', 'state', 'job', 'trans_num']

for col in categorical_cols:
    fraud_rates = df.groupby(col)['is_fraud'].mean()  # Compute fraud rate per category
    df[col] = df[col].map(fraud_rates)  # Replace categories with target encoding

# **Step 3: Drop Unnecessary Columns**
df.drop(columns=['street', 'trans_num'], inplace=True)  # Remove high-cardinality columns

# **Step 4: Scale Numerical Features (Excluding 'gender')**
scaler = StandardScaler()
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Ensure 'is_fraud' and 'gender' are not included in scaling
for col in ['is_fraud', 'gender']:
    if col in numerical_cols:
        numerical_cols.remove(col)

df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# # **Step 5: Train-Test Split**
# X = df.drop(columns=['is_fraud'])
# y = df['is_fraud']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # **Check Final Dataset**
# print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")
# print(df.head())

df.shape

df.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Compute the correlation matrix
correlation_matrix = df.corr()

# Plot the heatmap
plt.figure(figsize=(14, 10))  # Increase the figure size for more space
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", linewidths=0.5,
            fmt='.2f', annot_kws={"size": 12}, cbar_kws={'shrink': 0.8})  # Adjust annotations and colorbar size

# Adjusting the label size and adding space between labels
plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate and space x-axis labels
plt.yticks(rotation=0, fontsize=12)  # Adjust y-axis labels

# Title with space above
plt.title("Feature Correlation Heatmap", fontsize=16, pad=20)

# Adding extra space around the plot
plt.tight_layout()  # This adjusts the plot to ensure there's no overlap
plt.show()

# Set correlation threshold
threshold = 0.85

# Find feature pairs with high correlation
high_corr_pairs = np.where((correlation_matrix > threshold) & (correlation_matrix < 1))

# Print highly correlated feature pairs
for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):
    print(f"Feature {df.columns[i]} is highly correlated with Feature {df.columns[j]} (Corr: {correlation_matrix.iloc[i, j]:.2f})")

# Create a list of columns to drop based on high correlation
drop_columns = ['zip','unix_time','merch_lat','long','lat']

# Drop the highly correlated columns from the train and test datasets
df= df.drop(columns=drop_columns)

legit=df[df.is_fraud == 0]
fraud=df[df.is_fraud == 1]

print(legit.shape)
print(fraud.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from collections import Counter

# Split into features (X) and target (Y)
X = df.drop('is_fraud', axis=1)  # Features
Y = df['is_fraud']  # Target variable

# Step 1: Apply Random Undersampling (Reduce majority class)
undersample = RandomUnderSampler(sampling_strategy=0.1, random_state=42)
X_under, y_under = undersample.fit_resample(X, Y)

# Step 2: Split data into training and testing sets before applying SMOTE
X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.2, random_state=42, stratify=y_under)

# Step 3: Apply SMOTE only to the training set
oversample = SMOTE(sampling_strategy=0.5, random_state=42)
X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)

# Display class distributions
print("Original Data Distribution:", Counter(Y))
print("After Undersampling:", Counter(y_under))
print("After SMOTE on Training Set:", Counter(y_train_resampled))

import numpy as np

print("NaN values in X:", np.isnan(X).sum().sum())
print("Inf values in X:", np.isinf(X).sum().sum())

df.shape

df.head()

fraud_cases = df[df['is_fraud'] == 1]

# Display the first few fraud ca
fraud_cases.head()



# legit1=df[df.is_fraud == 0]
# fraud1=df[df.is_fraud == 1]

# print(legit1.shape)
# print(fraud1.shape)

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings

# Ignore FutureWarnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Get class distribution before and after resampling
original_counts = Counter(Y)  # Before any resampling
undersampled_counts = Counter(y_under)  # After undersampling
resampled_counts = Counter(y_train_resampled)  # After SMOTE on training set

# Create a figure with 3 subplots
plt.figure(figsize=(12, 5))

# Plot original class distribution
plt.subplot(1, 3, 1)  # 1 row, 3 columns, first plot
sns.barplot(x=list(original_counts.keys()), y=list(original_counts.values()), palette=['blue', 'red'])
plt.xticks(ticks=[0, 1], labels=['Legit (0)', 'Fraud (1)'])
plt.ylabel('Number of Transactions')
plt.title('Original Data')

# Plot after undersampling
plt.subplot(1, 3, 2)  # 1 row, 3 columns, second plot
sns.barplot(x=list(undersampled_counts.keys()), y=list(undersampled_counts.values()), palette=['blue', 'red'])
plt.xticks(ticks=[0, 1], labels=['Legit (0)', 'Fraud (1)'])
plt.ylabel('Number of Transactions')
plt.title('After Undersampling')

# Plot after SMOTE
plt.subplot(1, 3, 3)  # 1 row, 3 columns, third plot
sns.barplot(x=list(resampled_counts.keys()), y=list(resampled_counts.values()), palette=['blue', 'red'])
plt.xticks(ticks=[0, 1], labels=['Legit (0)', 'Fraud (1)'])
plt.ylabel('Number of Transactions')
plt.title('After SMOTE on Training Set')

# Show the plots
plt.tight_layout()
plt.show()

from collections import Counter

# Display class distribution after resampling
print("Training set class distribution (after SMOTE):", Counter(y_train_resampled))
print("Testing set class distribution (unchanged):", Counter(y_test))

import numpy as np
import pandas as pd

# Check data shapes
print("Original Training Data Shape:", X_train.shape)
print("Resampled Training Data Shape:", X_train_resampled.shape)
print("Test Data Shape:", X_test.shape)

# Verify if test data appears in training set (data leakage check)
train_rows = set([tuple(row) for row in X_train_resampled.to_numpy()])
test_rows = set([tuple(row) for row in X_test.to_numpy()])
overlap = train_rows.intersection(test_rows)

print("\nNumber of Overlapping Rows Between Train & Test:", len(overlap))
if len(overlap) > 0:
    print(" Data Leakage Detected! Test data exists in training set.")

# Check fraud distribution before and after resampling
print("\nFraud Distribution Before and After Resampling:")
print(f"Original Train Fraud Cases: {sum(y_train == 1)} / {len(y_train)} ({100 * sum(y_train == 1) / len(y_train):.2f}%)")
print(f"Resampled Train Fraud Cases: {sum(y_train_resampled == 1)} / {len(y_train_resampled)} ({100 * sum(y_train_resampled == 1) / len(y_train_resampled):.2f}%)")
print(f"Test Fraud Cases: {sum(y_test == 1)} / {len(y_test)} ({100 * sum(y_test == 1) / len(y_test):.2f}%)")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib

# # Load dataset
# X_train_resampled = joblib.load("X_train_resampled.pkl")
# y_train_resampled = joblib.load("y_train_resampled.pkl")
# X_test = joblib.load("X_test.pkl")
# y_test = joblib.load("y_test.pkl")

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
log_reg.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred_log = log_reg.predict(X_test)

# Metrics
print("\n Logistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print("Precision:", precision_score(y_test, y_pred_log))
print("Recall:", recall_score(y_test, y_pred_log))
print("F1-score:", f1_score(y_test, y_pred_log))

from sklearn.tree import DecisionTreeClassifier

# Train Decision Tree
dt = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)
dt.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred_dt = dt.predict(X_test)

# Metrics
print("\n Decision Tree Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt))
print("Recall:", recall_score(y_test, y_pred_dt))
print("F1-score:", f1_score(y_test, y_pred_dt))

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)
rf.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred_rf = rf.predict(X_test)

# Metrics
print("\n Random Forest Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Precision:", precision_score(y_test, y_pred_rf))
print("Recall:", recall_score(y_test, y_pred_rf))
print("F1-score:", f1_score(y_test, y_pred_rf))

from xgboost import XGBClassifier

# Train XGBoost
xgb_model = XGBClassifier(
    max_depth=6, learning_rate=0.05, n_estimators=200,
    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1,
    eval_metric='logloss', use_label_encoder=False, random_state=42
)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred_xgb = xgb_model.predict(X_test)

# Metrics
print("\n XGBoost Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Precision:", precision_score(y_test, y_pred_xgb))
print("Recall:", recall_score(y_test, y_pred_xgb))
print("F1-score:", f1_score(y_test, y_pred_xgb))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import joblib

# # Load dataset (Ensure that the training set used is the resampled one)
# X_train_resampled = joblib.load("X_train_resampled.pkl")  # Resampled training data
# y_train_resampled = joblib.load("y_train_resampled.pkl")  # Resampled training labels
# X_test = joblib.load("X_test.pkl")  # Original test set (untouched)
# y_test = joblib.load("y_test.pkl")  # Original test labels

# Define optimized models to prevent overfitting
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),
    "XGBoost": XGBClassifier(
        max_depth=6, learning_rate=0.05, n_estimators=200,
        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1,
        eval_metric='logloss', use_label_encoder=False, random_state=42
    )
}

# Dictionary to store results
results = {}

# Plot ROC Curve
plt.figure(figsize=(8, 6))

for name, model in models.items():
    # Train model using the resampled training data
    model.fit(X_train_resampled, y_train_resampled)

    # Predict on the training set (for accuracy comparison)
    y_train_pred = model.predict(X_train_resampled)
    train_accuracy = accuracy_score(y_train_resampled, y_train_pred)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Predict probabilities for ROC AUC calculation
    y_prob = model.predict_proba(X_test)[:, 1]

    # Compute evaluation metrics
    test_accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Compute ROC Curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    # Store results
    results[name] = {
        "Train Accuracy": train_accuracy,
        "Test Accuracy": test_accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1,
        "AUC": roc_auc
    }

    # Plot ROC Curve
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

# Finalize ROC Curve plot
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves for Different Models")
plt.legend()
plt.grid()
plt.show()

# Print the results in a structured way
print("\nPerformance Metrics for All Models:")
results_df = pd.DataFrame(results).T
print(results_df)

pip install lime

import sklearn
import sklearn.datasets
import sklearn.ensemble
import numpy as np
import lime
import lime.lime_tabular
from __future__ import print_function
np.random.seed(1)

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

explainer = lime.lime_tabular.LimeTabularExplainer(training_data=X_train.values,
                                                   feature_names=X_train.columns.values,
                                                   discretize_continuous=False,
                                                   class_names=["legit", "fraud"],
                                                   mode="classification",
                                                   verbose=True,
                                                   random_state=45)

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import numpy as np
import pandas as pd
import lime.lime_tabular
import shap
import matplotlib.pyplot as plt
import joblib
from IPython.display import display, FileLink

# Load trained model & data
# xgb_model = joblib.load("xgb_model.pkl")  # Replace with actual model file
# X_train = joblib.load("X_train.pkl")  # Load training data structure

# Feature names
features = X_train.columns.tolist()

def get_user_input():
    print("\nEnter values for the following features, separated by commas:")
    print(", ".join(features))

    user_input = input("\nEnter values: ")
    values = user_input.split(",")

    if len(values) != len(features):
        print(f"Error: Expected {len(features)} values, but got {len(values)}.")
        return None

    try:
        user_data = pd.DataFrame([list(map(float, values))], columns=features)
    except ValueError:
        print("Error: Ensure all inputs are numeric.")
        return None

    return user_data

# Get user input
user_sample = get_user_input()

if user_sample is not None:
    # Ensure input matches training data format
    user_sample = user_sample[X_train.columns]

    # Predict fraud probability
    fraud_probability = xgb_model.predict_proba(user_sample)[0][1]  # Probability of fraud
    threshold = 0.5  # Adjust threshold if needed
    prediction = 1 if fraud_probability > threshold else 0

    print(f"\nFraud Probability: {fraud_probability:.4f}")
    print("THE TRANSACTION OUTCOME IS:", "FRAUD" if prediction == 1 else "NOT FRAUD")
    print("\n")

    # LIME Explanation
    explainer_lime = lime.lime_tabular.LimeTabularExplainer(
        training_data=X_train.values,
        feature_names=X_train.columns.values,
        discretize_continuous=True,
        class_names=["legit", "fraud"],
        mode="classification",
        verbose=True,
        random_state=45
    )

    exp_lime = explainer_lime.explain_instance(user_sample.iloc[0], xgb_model.predict_proba)

    # Save LIME explanation to HTML
    lime_html_file = "lime_explanation.html"
    exp_lime.save_to_file(lime_html_file)
    print(f"LIME explanation saved as {lime_html_file}")

    # Provide download link
    display(FileLink(lime_html_file))

# import warnings
# warnings.simplefilter(action='ignore', category=FutureWarning)

# import numpy as np
# import pandas as pd
# import lime.lime_tabular
# import shap
# import matplotlib.pyplot as plt
# import joblib
# from IPython.display import display

# # Load trained model & data
# # xgb_model = joblib.load("xgb_model.pkl")  # Replace with actual model file
# # X_train = joblib.load("X_train.pkl")  # Load training data structure

# # Feature names
# features = X_train.columns.tolist()

# def get_user_input():
#     print("\nEnter values for the following features, separated by commas:")
#     print(", ".join(features))

#     user_input = input("\nEnter values: ")
#     values = user_input.split(",")

#     if len(values) != len(features):
#         print(f"Error: Expected {len(features)} values, but got {len(values)}.")
#         return None

#     try:
#         user_data = pd.DataFrame([list(map(float, values))], columns=features)
#     except ValueError:
#         print("Error: Ensure all inputs are numeric.")
#         return None

#     return user_data

# # Get user input
# user_sample = get_user_input()

# if user_sample is not None:
#     # Ensure input matches training data format
#     user_sample = user_sample[X_train.columns]

#     # Predict fraud probability
#     fraud_probability = xgb_model.predict_proba(user_sample)[0][1]  # Probability of fraud
#     threshold = 0.5  # Adjust threshold if needed
#     prediction = 1 if fraud_probability > threshold else 0

#     print(f"\nFraud Probability: {fraud_probability:.4f}")
#     print("THE TRANSACTION OUTCOME IS:", "FRAUD" if prediction == 1 else "NOT FRAUD")
#     print("\n")

#     # LIME Explanation
#     explainer_lime = lime.lime_tabular.LimeTabularExplainer(
#         training_data=X_train.values,
#         feature_names=X_train.columns.values,
#         discretize_continuous=True,
#         class_names=["legit", "fraud"],
#         mode="classification",
#         verbose=True,
#         random_state=45
#     )

#     exp_lime = explainer_lime.explain_instance(user_sample.iloc[0], xgb_model.predict_proba)
#     display(exp_lime.show_in_notebook(show_table=True))
#     print("\n")

#     exp_lime.as_pyplot_figure()
#     plt.show()
#     print("\n")

#     # SHAP Explanation
#     explainer_shap = shap.TreeExplainer(xgb_model)
#     shap_values = explainer_shap(user_sample)

#     # SHAP Summary Plot
#     shap.summary_plot(shap_values, user_sample, feature_names=features, show=False)
#     plt.show()
#     print("\n")

#     # SHAP Waterfall Plot
#     shap.plots.waterfall(shap_values[0], show=False)
#     plt.show()
#     print("\n")

#     # SHAP Force Plot
#     shap.force_plot(explainer_shap.expected_value, shap_values.values[0], user_sample.iloc[0], matplotlib=True)
#     plt.show()
#     print("\n")




import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import numpy as np
import pandas as pd
import lime.lime_tabular
import shap
import matplotlib.pyplot as plt
import joblib
from IPython.display import display, HTML

# Load trained model & data
# xgb_model = joblib.load("xgb_model.pkl")  # Replace with actual model file
# X_train = joblib.load("X_train.pkl")  # Load training data structure

# Feature names
features = X_train.columns.tolist()

def get_user_input():
    print("\nEnter values for the following features, separated by commas:")
    print(", ".join(features))

    user_input = input("\nEnter values: ")
    values = user_input.split(",")

    if len(values) != len(features):
        print(f"Error: Expected {len(features)} values, but got {len(values)}.")
        return None

    try:
        user_data = pd.DataFrame([list(map(float, values))], columns=features)
    except ValueError:
        print("Error: Ensure all inputs are numeric.")
        return None

    return user_data

# Get user input
user_sample = get_user_input()

if user_sample is not None:
    # Ensure input matches training data format
    user_sample = user_sample[X_train.columns]

    # Predict fraud probability
    fraud_probability = xgb_model.predict_proba(user_sample)[0][1]  # Probability of fraud
    threshold = 0.5  # Adjust threshold if needed
    prediction = 1 if fraud_probability > threshold else 0

    print(f"\nFraud Probability: {fraud_probability:.4f}")
    print("THE TRANSACTION OUTCOME IS:", "FRAUD" if prediction == 1 else "NOT FRAUD")
    print("\n")

    # LIME Explanation
    explainer_lime = lime.lime_tabular.LimeTabularExplainer(
        training_data=X_train.values,
        feature_names=X_train.columns.values,
        discretize_continuous=True,
        class_names=["legit", "fraud"],
        mode="classification",
        verbose=True,
        random_state=45
    )

    exp_lime = explainer_lime.explain_instance(user_sample.iloc[0], xgb_model.predict_proba)

    # Display explanation and feature importance together
    html = exp_lime.as_html()  # Get HTML version
    display(HTML(html))  # Display inline in Jupyter Notebook

    # Reduce spacing between table & explanation
    plt.figure(figsize=(8, 4))  # Adjust figure size
    exp_lime.as_pyplot_figure()
    plt.tight_layout()  # Reduce extra space
    plt.show()

    print("\n")

    # SHAP Explanation
    explainer_shap = shap.TreeExplainer(xgb_model)
    shap_values = explainer_shap(user_sample)

    # SHAP Summary Plot
    shap.summary_plot(shap_values, user_sample, feature_names=features, show=False)
    plt.show()
    print("\n")

    # SHAP Waterfall Plot
    shap.plots.waterfall(shap_values[0], show=False)
    plt.show()
    print("\n")

    # SHAP Force Plot
    shap.force_plot(explainer_shap.expected_value, shap_values.values[0], user_sample.iloc[0], matplotlib=True)
    plt.show()
    print("\n")

# -0.318827,1.324906,1,-0.079124,-0.289758,-0.291086,0.138289,0.657606,2019,1,2,1,6,1.007278 (fraud)
# -0.316766,-0.408741,0,-0.073467,-0.289758,-0.282429,-0.162994,0.594463,2019,1,1,0,0,1.667679 (legitimate)

import shap

print(X_test.shape)
print(X_train.shape)
# resampled_df.shape

# shap.summary_plot(shap_values, X_display, plot_type="bar")
explainer = shap.TreeExplainer(xgb_model)

shap_values.shape

shap.summary_plot(shap_values, X_test, plot_type="bar")

# Ensure feature names are taken from the DataFrame
feature_names = X_train.columns.tolist()  # Extract feature names
feature_names

import shap

# Initialize SHAP
shap.initjs()

# Convert X_test to DataFrame (if not already)
X_display = pd.DataFrame(X_test, columns=feature_names)

# Ensure explainer returns SHAP values in the correct format
shap_values = explainer(X_test)  # Generate SHAP values properly

shap.summary_plot(shap_values, X_display)

# import joblib

# # Save the resampled training data
# joblib.dump(X_train_resampled, "X_train_resampled.pkl")
# joblib.dump(y_train_resampled, "y_train_resampled.pkl")

# # Save the original test set
# joblib.dump(X_test, "X_test.pkl")
# joblib.dump(y_test, "y_test.pkl")

import pickle

# Save the trained model (e.g., xgb_model)
with open('xgb_model.pkl', 'wb') as file:
    pickle.dump(xgb_model, file)

from google.colab import files

# Download the file
files.download('xgb_model.pkl')

import joblib

joblib.dump(X_train, "X_train.pkl")
joblib.dump(feature_names, "feature_columns.pkl")